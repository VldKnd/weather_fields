{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Tuple,\n",
    "    Optional,\n",
    "    Literal\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from schedule import linear_beta_schedule\n",
    "from unet import Unet\n",
    "\n",
    "timesteps = 300\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a: torch.Tensor, t: torch.Tensor, x_shape: Tuple):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def q_sample(x_start: torch.Tensor, t: torch.Tensor, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def p_losses(\n",
    "        denoise_model: Unet,\n",
    "        x_start,\n",
    "        t,\n",
    "        noise=None,\n",
    "        loss_type=\"l1\",\n",
    "        self_condition: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "    if denoise_model.self_condition:\n",
    "        if self_condition is None:\n",
    "            raise RuntimeError(\"The self-conditioning is not provided. \")\n",
    "        \n",
    "        predicted_noise = denoise_model.forward(\n",
    "            x=x_noisy, \n",
    "            time=t,\n",
    "            x_self_cond=self_condition\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        predicted_noise = denoise_model.forward(\n",
    "            x=x_noisy, \n",
    "            time=t\n",
    "        )\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import (\n",
    "    Lambda,\n",
    "    Compose,\n",
    "    Resize\n",
    ")\n",
    "from torchvision.transforms import functional as TV_F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "class WeatherFieldsDataset(Dataset):\n",
    "    def __init__(self, root_dir, path_to_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        lr_data_folder = os.path.join(\n",
    "            root_dir, \n",
    "            path_to_folder,\n",
    "            \"train_2017_lr\",\n",
    "        )\n",
    "        hr_data_folder = os.path.join(\n",
    "            root_dir, \n",
    "            path_to_folder,\n",
    "            \"train_2017_hr\",\n",
    "        )\n",
    "        \n",
    "        date_idx_to_hr_file_names = {}\n",
    "\n",
    "        for hr_file_name in os.listdir(hr_data_folder):\n",
    "            hr_file_name_copy = hr_file_name\n",
    "            hr_file_name = hr_file_name.replace(\"_high_res_\", \"_\")\n",
    "            hr_file_name = hr_file_name.replace(\".npy\", \"\")\n",
    "            _, date, number = hr_file_name.split(\"_\")\n",
    "            index = int(\"\".join(date.split(\"-\")) + number)\n",
    "            date_idx_to_hr_file_names[index] = hr_file_name_copy\n",
    "\n",
    "        date_idx_to_file_pathes = {}\n",
    "\n",
    "        for lr_file_name in os.listdir(lr_data_folder):\n",
    "            lr_file_name_copy = lr_file_name\n",
    "            lr_file_name = lr_file_name.replace(\"_low_res_\", \"_\")\n",
    "            lr_file_name = lr_file_name.replace(\".npy\", \"\")\n",
    "            _, date, number = lr_file_name.split(\"_\")\n",
    "            index = int(\"\".join(date.split(\"-\")) + number)\n",
    "            hr_file_name = date_idx_to_hr_file_names.get(index)\n",
    "            if hr_file_name is not None:\n",
    "                date_idx_to_file_pathes[index] = (\n",
    "                    os.path.join(\n",
    "                        lr_data_folder,\n",
    "                        lr_file_name_copy,\n",
    "                    ),\n",
    "                    os.path.join(\n",
    "                        hr_data_folder,\n",
    "                        hr_file_name,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.transform = transform\n",
    "        self.date_idx_to_file_pathes = date_idx_to_file_pathes \n",
    "        self.sorted_date_idx = list(date_idx_to_file_pathes.keys())\n",
    "        self.sorted_date_idx.sort()\n",
    "        self.lr_transform = Compose([\n",
    "            Lambda(lambda t: (t / 255.)),\n",
    "            Lambda(lambda t: (t*2) - 1),\n",
    "            Lambda(lambda t: t.permute(2, 0, 1)),\n",
    "            Resize(size=(128, 128), antialias=True),\n",
    "        ])\n",
    "\n",
    "        self.hr_transform = Compose([\n",
    "            Lambda(lambda t: (t / 255.)),\n",
    "            Lambda(lambda t: (t*2) - 1),\n",
    "            Lambda(lambda t: t.permute(2, 0, 1))\n",
    "        ])\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.sorted_date_idx)\n",
    "\n",
    "    def __getitem__(self, index) -> torch.TensorType:\n",
    "        date_idx = self.sorted_date_idx[index]\n",
    "        lr_file_path, hr_file_path = self.date_idx_to_file_pathes[date_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lr_image = torch.from_numpy(np.load(lr_file_path))\n",
    "            hr_image = torch.from_numpy(np.load(hr_file_path))\n",
    "\n",
    "            lr_image = self.lr_transform(lr_image)\n",
    "            hr_image = self.hr_transform(hr_image)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                lr_image = TV_F.hflip(lr_image)\n",
    "                hr_image = TV_F.hflip(hr_image)\n",
    "                \n",
    "            if random.random() < 0.5:\n",
    "                lr_image = TV_F.vflip(lr_image)\n",
    "                hr_image = TV_F.vflip(hr_image)\n",
    "\n",
    "        return lr_image, hr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_generation import (\n",
    "    sample\n",
    ")\n",
    "from torch.optim import Adam\n",
    "from unet import Unet\n",
    "\n",
    "dataset = WeatherFieldsDataset(\n",
    "    root_dir=os.path.abspath(\"..\"),\n",
    "    path_to_folder=os.path.join(\n",
    "        \"data\",\n",
    "        \"wrf_data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_size = 128\n",
    "channels = 3\n",
    "\n",
    "hr_image, lr_image = dataset[0]\n",
    "C, H, W = hr_image.shape\n",
    "\n",
    "model = Unet(\n",
    "    dim=H,\n",
    "    channels=C,\n",
    "    dim_mults=(1, 2, 4,),\n",
    "    self_condition=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def spectral_noise_generator(shape: Tuple) -> torch.Tensor:\n",
    "    noise = torch.randn(shape)\n",
    "    return noise, torch.fft.rfft2(noise)\n",
    "\n",
    "def complex_mse_loss(  \n",
    "        input,\n",
    "        target,\n",
    "    ):\n",
    "    difference = input - target\n",
    "    return ((difference.real**2 + difference.imag**2) / 2).mean()\n",
    "\n",
    "def p_spectral_losses(\n",
    "        denoise_model: Unet,\n",
    "        x_start: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        noise: Optional[torch.Tensor] = None,\n",
    "        self_condition: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        domain_fourier_noise = torch.randn_like(x_start)\n",
    "        fourier_noise = torch.fft.rfft2(domain_fourier_noise)\n",
    "        \n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    x_noisy_transformed = q_sample(x_start=x_start, t=t, noise=domain_fourier_noise)\n",
    "\n",
    "    if denoise_model.self_condition:\n",
    "        if self_condition is None:\n",
    "            raise RuntimeError(\"The self-conditioning is not provided. \")\n",
    "        \n",
    "        predicted_noise = denoise_model.forward(\n",
    "            x=x_noisy, \n",
    "            time=t,\n",
    "            x_self_cond=self_condition\n",
    "        )\n",
    "        \n",
    "        predicted_domain_fourier_noise = denoise_model.forward(\n",
    "            x=x_noisy_transformed, \n",
    "            time=t,\n",
    "            x_self_cond=self_condition\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        predicted_noise = denoise_model.forward(\n",
    "            x=x_noisy, \n",
    "            time=t\n",
    "        )\n",
    "        \n",
    "        predicted_domain_fourier_noise = denoise_model.forward(\n",
    "            x=x_noisy_transformed, \n",
    "            time=t,\n",
    "            x_self_cond=self_condition\n",
    "        )\n",
    "\n",
    "    loss = (\n",
    "        F.mse_loss(noise, predicted_noise) +\n",
    "        complex_mse_loss(fourier_noise, torch.fft.rfft2(predicted_domain_fourier_noise))\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18686.611328125\n",
      "Loss: 135340.046875\n",
      "Loss: 19188.12109375\n",
      "Loss: 19802.421875\n",
      "Loss: 84844.78125\n",
      "Loss: 46212.484375\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "state = {\n",
    "   \"loss_train\":[]\n",
    "}\n",
    "for epoch in range(epochs):\n",
    "    for step, (lr_batch, hr_batch) in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_size, _, _, _ = lr_batch.shape\n",
    "      lr_batch = lr_batch.to(device)\n",
    "      hr_batch = hr_batch.to(device)\n",
    "\n",
    "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "      loss = p_spectral_losses(\n",
    "         denoise_model=model, \n",
    "         x_start=hr_batch, \n",
    "         t=t,\n",
    "         self_condition=lr_batch,\n",
    "      )\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      state['loss_train'].append(float(loss.detach().cpu()))\n",
    "      optimizer.step()\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now = now.strftime('%m_%d_%M_%S')\n",
    "file_name = now + \"_checkpoint.pkl\"\n",
    "\n",
    "pwd_path = os.path.abspath(\"..\")\n",
    "folder_path = os.path.join(\n",
    "    pwd_path,\n",
    "    \"checkpoints\"\n",
    ")\n",
    "\n",
    "file_path = os.path.join(\n",
    "    folder_path,\n",
    "    file_name\n",
    ")\n",
    "\n",
    "state[\"model_state_dict\"] = model.state_dict()\n",
    "state[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "state[\"model_kwargs\"] = {\n",
    "    \"dim\":H,\n",
    "    \"channels\":C,\n",
    "    \"dim_mults\":(1, 2, 4,),\n",
    "    \"self_condition\":True,\n",
    "}\n",
    "state[\"other\"] = {\n",
    "    \"epochs\":epochs,\n",
    "    \"batch_size\":batch_size,\n",
    "}\n",
    "torch.save(state, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shad-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
