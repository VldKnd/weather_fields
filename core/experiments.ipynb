{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from inspect import isfunction\n",
    "from utils import exists, default\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "def _warmup_beta(linear_start, linear_end, n_timestep, warmup_frac):\n",
    "    betas = linear_end * np.ones(n_timestep, dtype=np.float64)\n",
    "    warmup_time = int(n_timestep * warmup_frac)\n",
    "    betas[:warmup_time] = np.linspace(\n",
    "        linear_start, linear_end, warmup_time, dtype=np.float64)\n",
    "    return betas\n",
    "\n",
    "\n",
    "def make_beta_schedule(\n",
    "        schedule_type, \n",
    "        n_timestep, \n",
    "        linear_start=1e-4, \n",
    "        linear_end=2e-2, \n",
    "        cosine_s=8e-3\n",
    "    ):\n",
    "    if schedule_type == 'quad':\n",
    "        betas = np.linspace(linear_start ** 0.5, linear_end ** 0.5,\n",
    "                            n_timestep, dtype=np.float64) ** 2\n",
    "    elif schedule_type == 'linear':\n",
    "        betas = np.linspace(linear_start, linear_end,\n",
    "                            n_timestep, dtype=np.float64)\n",
    "    elif schedule_type == 'warmup10':\n",
    "        betas = _warmup_beta(linear_start, linear_end,\n",
    "                             n_timestep, 0.1)\n",
    "    elif schedule_type == 'warmup50':\n",
    "        betas = _warmup_beta(linear_start, linear_end,\n",
    "                             n_timestep, 0.5)\n",
    "    elif schedule_type == 'const':\n",
    "        betas = linear_end * np.ones(n_timestep, dtype=np.float64)\n",
    "\n",
    "    elif schedule_type == 'jsd':\n",
    "        betas = 1. / np.linspace(n_timestep,\n",
    "                                 1, n_timestep, dtype=np.float64)\n",
    "        \n",
    "    elif schedule_type == \"cosine\":\n",
    "        timesteps = (\n",
    "            torch.arange(n_timestep + 1, dtype=torch.float64) /\n",
    "            n_timestep + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * math.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = betas.clamp(max=0.999)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(schedule_type)\n",
    "    return betas\n",
    "\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def complex_mse_loss(  \n",
    "        input,\n",
    "        target,\n",
    "    ):\n",
    "    difference = input - target\n",
    "    return ((difference.real**2 + difference.imag**2) / 2).mean()\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        denoise_fn: nn.Module,\n",
    "        image_size: int,\n",
    "        channels:int = 3,\n",
    "        loss_type:str ='l2',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.denoise_fn = denoise_fn\n",
    "\n",
    "    def set_new_noise_schedule(\n",
    "            self, \n",
    "            device,\n",
    "            schedule_type, \n",
    "            n_timestep, \n",
    "            linear_start=1e-4, \n",
    "            linear_end=2e-2, \n",
    "            cosine_s=8e-3,\n",
    "        ):\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32, device=device)\n",
    "\n",
    "        betas = make_beta_schedule(\n",
    "            schedule_type=schedule_type,\n",
    "            n_timestep=n_timestep,\n",
    "            linear_start=linear_start,\n",
    "            linear_end=linear_end,\n",
    "            cosine_s=cosine_s\n",
    "        )\n",
    "\n",
    "        betas = betas.detach().cpu().numpy() \\\n",
    "            if isinstance(betas, torch.Tensor) else betas\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "        self.sqrt_alphas_cumprod_prev = np.sqrt(np.append(1., alphas_cumprod))\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas))\n",
    "        \n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "\n",
    "        self.register_buffer('alphas_cumprod_prev',\n",
    "                             to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        self.register_buffer('sqrt_alphas_cumprod',\n",
    "                             to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod',\n",
    "                             to_torch(np.sqrt(1. - alphas_cumprod)))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod',\n",
    "                             to_torch(np.log(1. - alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod',\n",
    "                             to_torch(np.sqrt(1. / alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod',\n",
    "                             to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
    "\n",
    "        posterior_variance = betas * \\\n",
    "            (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "        \n",
    "        self.register_buffer('posterior_variance',\n",
    "                             to_torch(posterior_variance))\n",
    "        \n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(\n",
    "            np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        \n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        \n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return self.sqrt_recip_alphas_cumprod[t] * x_t - \\\n",
    "            self.sqrt_recipm1_alphas_cumprod[t] * noise\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = self.posterior_mean_coef1[t] * \\\n",
    "            x_start + self.posterior_mean_coef2[t] * x_t\n",
    "        posterior_log_variance_clipped = self.posterior_log_variance_clipped[t]\n",
    "        return posterior_mean, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised: bool, condition_x=None):\n",
    "        batch_size = x.shape[0]\n",
    "        noise_level = torch.FloatTensor(\n",
    "            [self.sqrt_alphas_cumprod_prev[t+1]]\n",
    "        ).repeat(batch_size, 1).to(x.device)\n",
    "        \n",
    "        if condition_x is not None:\n",
    "            predicted_noise = self.denoise_fn(\n",
    "                torch.cat([condition_x, x], dim=1),\n",
    "                noise_level\n",
    "            )\n",
    "            \n",
    "            x_recon = self.predict_start_from_noise(\n",
    "                x, t=t, noise=predicted_noise\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            x_recon = self.predict_start_from_noise(\n",
    "                x, t=t, noise=self.denoise_fn(x, noise_level)\n",
    "            )\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_log_variance = self.q_posterior(\n",
    "            x_start=x_recon, x_t=x, t=t\n",
    "        )\n",
    "        \n",
    "        return model_mean, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, clip_denoised=True, condition_x=None):\n",
    "        model_mean, model_log_variance = self.p_mean_variance(\n",
    "            x=x, \n",
    "            t=t, \n",
    "            clip_denoised=clip_denoised, \n",
    "            condition_x=condition_x\n",
    "        )\n",
    "        noise = torch.randn_like(x) if t > 0 else torch.zeros_like(x)\n",
    "        return model_mean + noise * (0.5 * model_log_variance).exp()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, x_in):\n",
    "        device = self.betas.device\n",
    "\n",
    "        x = x_in\n",
    "        shape = x.shape\n",
    "        img = torch.randn(shape, device=device)\n",
    "        for i in reversed(range(0, self.num_timesteps)):\n",
    "            img = self.p_sample(img, i, condition_x=x)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=1, continous=False):\n",
    "        image_size = self.image_size\n",
    "        channels = self.channels\n",
    "        return self.p_sample_loop(\n",
    "            (\n",
    "                batch_size, \n",
    "                channels, \n",
    "                image_size, \n",
    "                image_size\n",
    "            ), \n",
    "            continous\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def super_resolution(self, x_in):\n",
    "        return self.p_sample_loop(x_in)\n",
    "\n",
    "    def q_sample(self, x_start, continuous_sqrt_alpha_cumprod, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start, device=DEVICE))\n",
    "\n",
    "        return (\n",
    "            continuous_sqrt_alpha_cumprod * x_start +\n",
    "            (1 - continuous_sqrt_alpha_cumprod**2).sqrt() * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_in, noise=None):\n",
    "        x_start = x_in['HR']\n",
    "        B, C, H, W = x_start.shape\n",
    "        t = np.random.randint(1, self.num_timesteps + 1)\n",
    "\n",
    "        continuous_sqrt_alpha_cumprod = torch.FloatTensor(\n",
    "            np.random.uniform(\n",
    "                self.sqrt_alphas_cumprod_prev[t-1],\n",
    "                self.sqrt_alphas_cumprod_prev[t],\n",
    "                size=B\n",
    "            )\n",
    "        )\n",
    "\n",
    "        continuous_sqrt_alpha_cumprod = continuous_sqrt_alpha_cumprod.view(B, -1)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start, device=DEVICE))\n",
    "        continuous_sqrt_alpha_cumprod = continuous_sqrt_alpha_cumprod.to(DEVICE)\n",
    "\n",
    "        x_noisy = self.q_sample(\n",
    "            x_start=x_start, \n",
    "            continuous_sqrt_alpha_cumprod=continuous_sqrt_alpha_cumprod.view(-1, 1, 1, 1), \n",
    "            noise=noise\n",
    "        )\n",
    "\n",
    "        noise_recon = self.denoise_fn(\n",
    "            torch.cat([x_in['LR'], x_noisy], dim=1), \n",
    "            continuous_sqrt_alpha_cumprod\n",
    "        )\n",
    "\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = F.l1_loss(\n",
    "                noise, \n",
    "                noise_recon\n",
    "            )\n",
    "\n",
    "        elif self.loss_type == 'l2':\n",
    "            loss = F.mse_loss(\n",
    "                noise, \n",
    "                noise_recon\n",
    "            )\n",
    "\n",
    "        elif self.loss_type == 'fl2':\n",
    "            with torch.no_grad():\n",
    "                fourier_noise = ( 2**(1/2) / H ) * rfft2(noise)\n",
    "\n",
    "            loss = complex_mse_loss(\n",
    "                fourier_noise, \n",
    "                ( 2**(1/2) / H ) * rfft2(noise_recon)\n",
    "            )\n",
    "\n",
    "        elif self.loss_type == 'fl2_l2':\n",
    "            with torch.no_grad():\n",
    "                fourier_noise = ( 2**(1/2) / H ) * rfft2(noise)\n",
    "\n",
    "            loss = F.mse_loss(\n",
    "                noise, \n",
    "                noise_recon\n",
    "            ) + complex_mse_loss(\n",
    "                fourier_noise, \n",
    "                ( 2**(1/2) / H ) * rfft2(noise_recon)\n",
    "            )\n",
    "\n",
    "        elif self.loss_type == 'sl1_l2':\n",
    "            with torch.no_grad():\n",
    "                fourier_noise = ( 2**(1/2) / H ) * rfft2(noise)\n",
    "\n",
    "            x_noisy_prev = self.q_sample(\n",
    "                x_start=x_start, \n",
    "                continuous_sqrt_alpha_cumprod=self.sqrt_alphas_cumprod_prev[t-1].view(B, -1), \n",
    "                noise=noise\n",
    "            )\n",
    "\n",
    "            x_recon_prev = self.sqrt_recip_alphas_cumprod[t] * x_noisy - \\\n",
    "                self.sqrt_recipm1_alphas_cumprod[t] * noise\n",
    "            \n",
    "            loss = F.mse_loss(\n",
    "                noise, \n",
    "                noise_recon\n",
    "            ) + self.sqrt_alphas_cumprod_prev[t-1].view(B, -1) * \\\n",
    "            self.torch.abs(\n",
    "                torch.pow(x_noisy_prev.real, 2) + \n",
    "                torch.pow(x_noisy_prev.imag, 2) - \n",
    "                torch.pow(x_recon_prev.real, 2) + \n",
    "                torch.pow(x_recon_prev.imag, 2)\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.p_losses(x, *args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shad-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
